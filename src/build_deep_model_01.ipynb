{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "from skimage import feature\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(dl):\n",
    "    '''\n",
    "    Calculates mean and std for each channel (RGB).\n",
    "    Data has 4 dims [batch, C, H, W] i.e. [0, 1, 2, 3] and we want to calculate\n",
    "    mean/std across (dims 0, 2, 3) but for each channel (dim 1).\n",
    "    mean = sum (across all batches per channel) / n_batches\n",
    "    std = sqrt(variance), \n",
    "        where variance = E(X**2) - E(X)**2\n",
    "        where E(x) is expected value of x i.e. mean(x) \n",
    "        variance = mean(squared(data) per channel) - squared(mean(data per channel))\n",
    "    '''\n",
    "    n_batches = len(dl)\n",
    "    batch_expected_x = 0\n",
    "    batch_expected_x_sq = 0\n",
    "    for data, _ in dl:\n",
    "        batch_expected_x_sq += torch.mean(data**2, dim=[0, 2, 3]) # E(X**2) batch                                \n",
    "        batch_expected_x    += torch.mean(data, dim=[0, 2, 3]) # E(X) batch\n",
    "    mean = batch_expected_x / n_batches # overall E(X)\n",
    "    var = (batch_expected_x_sq / n_batches) - mean**2 # E(X**2) - E(X)**2\n",
    "    std = torch.sqrt(var)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mt/Library/Python/3.9/lib/python/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = datasets.ImageFolder(root='/Users/mt/data/Landmark_Classification/train',\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.Resize((128, 128)),\n",
    "                                        # transforms.CenterCrop(128),                                    \n",
    "                                        # transforms.RandomHorizontalFlip(p=0.25),\n",
    "                                        transforms.ToTensor()]))\n",
    "train_dl = data.DataLoader(train_ds, batch_size=batch_size)\n",
    "\n",
    "mean, std = get_mean_std(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4875, 0.5093, 0.4983]), tensor([0.2797, 0.2733, 0.3129]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_mean = tensor([-2.1115e-07,  1.8072e-07, -3.0436e-07])\n",
      "norm_std = tensor([1.0000, 1.0000, 1.0000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Normalize using the statistics obtained above and apply images augumentations. \n",
    "#\n",
    "train_ds = datasets.ImageFolder(root='/Users/mt/data/Landmark_Classification/train',\n",
    "                                transform=transforms.Compose([                                \n",
    "                                    transforms.Resize((128, 128)),\n",
    "                                    # transforms.CenterCrop(128),                                    \n",
    "                                    # transforms.RandomHorizontalFlip(p=0.25),\n",
    "                                    transforms.ToTensor(),                                    \n",
    "                                    transforms.Normalize(mean, std)]))\n",
    "classes = train_ds.classes\n",
    "train_dl = data.DataLoader(train_ds, batch_size=batch_size)\n",
    "\n",
    "#\n",
    "# Compute mean and std after normalization\n",
    "#\n",
    "norm_mean, norm_std = get_mean_std(train_dl)\n",
    "print(f'norm_mean = {norm_mean}')\n",
    "print(f'norm_std = {norm_std}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Define layers of a CNN\n",
    "        self.conv_1= nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu_1 = nn.ReLU(inplace=True)\n",
    "        self.bn_1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2)        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu_2 = nn.ReLU(inplace=True)\n",
    "        self.bn_2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=2)        \n",
    "        self.conv_3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu_3 = nn.ReLU(inplace=True) \n",
    "        self.bn_3 = nn.BatchNorm2d(64)        \n",
    "        self.maxpool_3 = nn.MaxPool2d(kernel_size=2)        \n",
    "        self.conv_4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu_4 = nn.ReLU(inplace=True)\n",
    "        self.bn_4 = nn.BatchNorm2d(128)        \n",
    "        self.maxpool_4 = nn.MaxPool2d(kernel_size=2)                \n",
    "        self.conv_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.relu_5 = nn.ReLU(inplace=True)\n",
    "        self.bn_5 = nn.BatchNorm2d(256)        \n",
    "        self.maxpool_5 = nn.MaxPool2d(kernel_size=2)                        \n",
    "        self.flatten_1 = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(in_features=4 * 4 * 256, out_features=1024)\n",
    "        self.relu_6 = nn.ReLU(inplace=True)\n",
    "        self.bn_6 = nn.BatchNorm1d(1024)        \n",
    "        self.linear_2 = nn.Linear(in_features=1024, out_features=50)\n",
    "        self.output = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        ## Layers list\n",
    "        self.layers = [layer for layer in self.modules() if not isinstance(layer, Net)]\n",
    "      \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)        \n",
    "        return x\n",
    "\n",
    "# instantiate the CNN\n",
    "model = Net()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    n_sample = len(train_dl)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_dl):        \n",
    "            output = model(data)\n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            optimizer.zero_grad()            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item() \n",
    "            \n",
    "        else:\n",
    "            avg_loss = train_loss / n_sample                                 \n",
    "            print('epoch', epoch, 'loss', avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.023807676884276357\n",
      "epoch 2 loss 0.020727139393590947\n",
      "epoch 3 loss 0.018057170175412234\n",
      "epoch 4 loss 0.01612152494093839\n",
      "epoch 5 loss 0.014308119049057998\n",
      "epoch 6 loss 0.012865767167742424\n",
      "epoch 7 loss 0.011694732345046198\n",
      "epoch 8 loss 0.01065020253535043\n",
      "epoch 9 loss 0.009755415321783499\n",
      "epoch 10 loss 0.008934328700366251\n",
      "epoch 11 loss 0.008258165717323093\n",
      "epoch 12 loss 0.007644926716741967\n",
      "epoch 13 loss 0.0070918997566394035\n",
      "epoch 14 loss 0.006581640425831713\n",
      "epoch 15 loss 0.006164277236824459\n",
      "epoch 16 loss 0.005741489561958278\n",
      "epoch 17 loss 0.005384536023484543\n",
      "epoch 18 loss 0.005051236556337631\n",
      "epoch 19 loss 0.004946113435893339\n",
      "epoch 20 loss 0.004641354500756975\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "train(num_epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = datasets.ImageFolder(root='/Users/mt/data/Landmark_Classification/test',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    transforms.Resize((128, 128)),                                   \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)]))\n",
    "\n",
    "test_dl = data.DataLoader(test_ds, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.849874\n",
      "\n",
      "\n",
      "Test Accuracy: 44% (670/1500)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.\n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "# set the module to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_dl):\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(output, target)\n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - test_loss))\n",
    "    # convert output probabilities to predicted class\n",
    "    output = torch.exp(output)  # Get probability from LogSoftmax\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    # compare predictions to true label\n",
    "    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "    total += data.size(0)\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "    100. * correct / total, correct, total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/cnn.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
